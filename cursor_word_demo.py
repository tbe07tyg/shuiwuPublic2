#!/usr/bin/env python3
"""
Cursorç¯å¢ƒä¸­çš„Wordæ–‡æ¡£è¯†åˆ«åŠŸèƒ½æ¼”ç¤º
ç›´æ¥åœ¨å½“å‰å¯¹è¯ä¸­ä½¿ç”¨ï¼Œæ— éœ€é¢å¤–é…ç½®
"""

import os
from docx import Document
import json

def list_word_documents():
    """åˆ—å‡ºå½“å‰ç›®å½•ä¸­çš„æ‰€æœ‰Wordæ–‡æ¡£"""
    word_files = []
    for file in os.listdir('.'):
        if file.endswith('.docx') and not file.startswith('~'):
            word_files.append(file)
    return word_files

def analyze_word_document(filename):
    """åˆ†æWordæ–‡æ¡£å†…å®¹"""
    try:
        doc = Document(filename)
        
        analysis = {
            "æ–‡æ¡£åç§°": filename,
            "åŸºæœ¬ä¿¡æ¯": {
                "æ®µè½æ•°": len(doc.paragraphs),
                "è¡¨æ ¼æ•°": len(doc.tables),
                "æ€»å­—ç¬¦æ•°": sum(len(p.text) for p in doc.paragraphs)
            },
            "å†…å®¹é¢„è§ˆ": [],
            "è¡¨æ ¼æ•°æ®": []
        }
        
        # æå–å‰5ä¸ªéç©ºæ®µè½
        content_count = 0
        for para in doc.paragraphs:
            if para.text.strip() and content_count < 5:
                analysis["å†…å®¹é¢„è§ˆ"].append({
                    "æ ·å¼": para.style.name,
                    "å†…å®¹": para.text.strip()
                })
                content_count += 1
        
        # æå–è¡¨æ ¼æ•°æ®
        for i, table in enumerate(doc.tables):
            table_data = []
            for row in table.rows:
                row_data = [cell.text.strip() for cell in row.cells]
                table_data.append(row_data)
            
            analysis["è¡¨æ ¼æ•°æ®"].append({
                "è¡¨æ ¼åºå·": i + 1,
                "å°ºå¯¸": f"{len(table.rows)}è¡Œ x {len(table.columns)}åˆ—",
                "æ•°æ®": table_data[:3]  # åªæ˜¾ç¤ºå‰3è¡Œ
            })
        
        return analysis
    
    except Exception as e:
        return {"é”™è¯¯": f"æ— æ³•åˆ†ææ–‡æ¡£: {str(e)}"}

def demonstrate_word_processing():
    """æ¼”ç¤ºWordæ–‡æ¡£å¤„ç†åŠŸèƒ½"""
    print("ğŸ” Cursorä¸­çš„Wordæ–‡æ¡£è¯†åˆ«åŠŸèƒ½æ¼”ç¤º")
    print("=" * 60)
    
    # 1. åˆ—å‡ºæ‰€æœ‰Wordæ–‡æ¡£
    word_files = list_word_documents()
    
    if not word_files:
        print("âŒ å½“å‰ç›®å½•ä¸­æ²¡æœ‰æ‰¾åˆ°Wordæ–‡æ¡£")
        print("ğŸ’¡ æ‚¨å¯ä»¥:")
        print("   1. ä¸Šä¼ ä¸€ä¸ªWordæ–‡æ¡£åˆ°å½“å‰ç›®å½•")
        print("   2. æˆ–è€…è¯·æ±‚æˆ‘åˆ›å»ºä¸€ä¸ªæ¼”ç¤ºæ–‡æ¡£")
        return
    
    print(f"ğŸ“„ å‘ç° {len(word_files)} ä¸ªWordæ–‡æ¡£:")
    for i, file in enumerate(word_files, 1):
        size = os.path.getsize(file) / 1024  # KB
        print(f"   {i}. {file} ({size:.1f} KB)")
    
    print("\n" + "="*60)
    
    # 2. åˆ†æç¬¬ä¸€ä¸ªæ–‡æ¡£
    target_file = word_files[0]
    print(f"ğŸ“– æ­£åœ¨åˆ†æ: {target_file}")
    print("-" * 40)
    
    analysis = analyze_word_document(target_file)
    
    # 3. æ˜¾ç¤ºåˆ†æç»“æœ
    if "é”™è¯¯" in analysis:
        print(f"âŒ {analysis['é”™è¯¯']}")
        return
    
    print("âœ… åˆ†æå®Œæˆï¼")
    print(json.dumps(analysis, ensure_ascii=False, indent=2))
    
    print("\n" + "="*60)
    print("ğŸ¯ åœ¨Cursorä¸­ä½¿ç”¨Wordæ–‡æ¡£è¯†åˆ«åŠŸèƒ½çš„æ–¹æ³•:")
    print("   1. ç›´æ¥åœ¨å¯¹è¯ä¸­è¯·æ±‚: 'åˆ†æè¿™ä¸ªWordæ–‡æ¡£'")
    print("   2. å…·ä½“æ“ä½œè¯·æ±‚: 'æå–æ–‡æ¡£ä¸­çš„è¡¨æ ¼æ•°æ®'") 
    print("   3. ç¼–è¾‘è¯·æ±‚: 'åœ¨æ–‡æ¡£ä¸­æ·»åŠ ä¸€ä¸ªæ–°æ®µè½'")
    print("   4. æ ¼å¼åŒ–è¯·æ±‚: 'å°†æ ‡é¢˜æ”¹ä¸ºç²—ä½“'")

if __name__ == "__main__":
    demonstrate_word_processing() 