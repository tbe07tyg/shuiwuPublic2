#!/usr/bin/env python3
"""
Cursor Wordæ–‡æ¡£åŠ©æ‰‹
ç›´æ¥åœ¨Cursorå¯¹è¯ä¸­æä¾›Wordæ–‡æ¡£è¯†åˆ«å’Œå¤„ç†åŠŸèƒ½
æ— éœ€å¤æ‚çš„MCPé…ç½®ï¼Œç›´æ¥å¯ç”¨
"""

import os
import json
from docx import Document
from pathlib import Path

class CursorWordHelper:
    """Cursorç¯å¢ƒä¸‹çš„Wordæ–‡æ¡£å¤„ç†åŠ©æ‰‹"""
    
    def __init__(self, workspace_path="."):
        self.workspace_path = workspace_path
        
    def find_word_documents(self):
        """åœ¨å·¥ä½œåŒºä¸­æŸ¥æ‰¾æ‰€æœ‰Wordæ–‡æ¡£"""
        word_files = []
        for root, dirs, files in os.walk(self.workspace_path):
            for file in files:
                if file.endswith('.docx') and not file.startswith('~'):
                    full_path = os.path.join(root, file)
                    rel_path = os.path.relpath(full_path, self.workspace_path)
                    size_kb = os.path.getsize(full_path) / 1024
                    word_files.append({
                        'name': file,
                        'path': rel_path,
                        'full_path': full_path,
                        'size_kb': round(size_kb, 1)
                    })
        return word_files
    
    def analyze_document(self, file_path):
        """æ·±åº¦åˆ†æWordæ–‡æ¡£"""
        try:
            doc = Document(file_path)
            
            analysis = {
                "æ–‡æ¡£è·¯å¾„": file_path,
                "åŸºæœ¬ä¿¡æ¯": {
                    "æ®µè½æ€»æ•°": len(doc.paragraphs),
                    "è¡¨æ ¼æ€»æ•°": len(doc.tables),
                    "å›¾ç‰‡æ€»æ•°": self._count_images(doc),
                    "æ€»å­—ç¬¦æ•°": sum(len(p.text) for p in doc.paragraphs),
                    "éç©ºæ®µè½æ•°": len([p for p in doc.paragraphs if p.text.strip()])
                },
                "æ–‡æ¡£ç»“æ„": self._extract_structure(doc),
                "å†…å®¹æ‘˜è¦": self._extract_summary(doc),
                "è¡¨æ ¼æ•°æ®": self._extract_tables(doc),
                "æ ·å¼ç»Ÿè®¡": self._analyze_styles(doc)
            }
            
            return analysis
            
        except Exception as e:
            return {"é”™è¯¯": f"æ— æ³•åˆ†ææ–‡æ¡£ {file_path}: {str(e)}"}
    
    def _count_images(self, doc):
        """ç»Ÿè®¡æ–‡æ¡£ä¸­çš„å›¾ç‰‡æ•°é‡"""
        count = 0
        for paragraph in doc.paragraphs:
            for run in paragraph.runs:
                if hasattr(run, '_element') and run._element.xpath('.//a:blip'):
                    count += 1
        return count
    
    def _extract_structure(self, doc):
        """æå–æ–‡æ¡£ç»“æ„"""
        structure = []
        for i, para in enumerate(doc.paragraphs):
            if para.text.strip():
                structure.append({
                    "æ®µè½å·": i + 1,
                    "æ ·å¼": para.style.name,
                    "çº§åˆ«": self._get_heading_level(para.style.name),
                    "å†…å®¹": para.text.strip()[:100] + ("..." if len(para.text.strip()) > 100 else "")
                })
        return structure
    
    def _get_heading_level(self, style_name):
        """è·å–æ ‡é¢˜çº§åˆ«"""
        if 'Heading' in style_name:
            try:
                return int(style_name.split()[-1])
            except:
                return 0
        elif style_name == 'Title':
            return 0
        return None
    
    def _extract_summary(self, doc):
        """æå–æ–‡æ¡£æ‘˜è¦"""
        summary = {
            "æ ‡é¢˜": "",
            "ä¸»è¦å†…å®¹": [],
            "å…³é”®ä¿¡æ¯": []
        }
        
        # æå–æ ‡é¢˜
        for para in doc.paragraphs:
            if para.style.name in ['Title', 'Heading 1']:
                summary["æ ‡é¢˜"] = para.text.strip()
                break
        
        # æå–ä¸»è¦æ®µè½
        content_paras = [p for p in doc.paragraphs if p.text.strip() and 
                        p.style.name not in ['Title', 'Heading 1', 'Heading 2', 'Heading 3']]
        summary["ä¸»è¦å†…å®¹"] = [p.text.strip() for p in content_paras[:3]]
        
        # æå–å…³é”®ä¿¡æ¯ï¼ˆæ•°å­—ã€æ—¥æœŸç­‰ï¼‰
        import re
        all_text = " ".join([p.text for p in doc.paragraphs])
        numbers = re.findall(r'\d+', all_text)
        summary["å…³é”®ä¿¡æ¯"] = {
            "æ•°å­—ç»Ÿè®¡": len(numbers),
            "åŒ…å«æ•°å­—": numbers[:10] if numbers else []
        }
        
        return summary
    
    def _extract_tables(self, doc):
        """æå–è¡¨æ ¼æ•°æ®"""
        tables_data = []
        for i, table in enumerate(doc.tables):
            table_info = {
                "è¡¨æ ¼åºå·": i + 1,
                "å°ºå¯¸": f"{len(table.rows)}è¡Œ x {len(table.columns)}åˆ—",
                "è¡¨å¤´": [],
                "æ•°æ®é¢„è§ˆ": []
            }
            
            if table.rows:
                # æå–è¡¨å¤´
                header_row = table.rows[0]
                table_info["è¡¨å¤´"] = [cell.text.strip() for cell in header_row.cells]
                
                # æå–æ•°æ®é¢„è§ˆï¼ˆå‰5è¡Œï¼‰
                for row_idx, row in enumerate(table.rows[1:6], 1):
                    row_data = [cell.text.strip() for cell in row.cells]
                    table_info["æ•°æ®é¢„è§ˆ"].append({
                        f"ç¬¬{row_idx}è¡Œ": row_data
                    })
            
            tables_data.append(table_info)
        
        return tables_data
    
    def _analyze_styles(self, doc):
        """åˆ†ææ–‡æ¡£æ ·å¼ä½¿ç”¨æƒ…å†µ"""
        style_count = {}
        for para in doc.paragraphs:
            style_name = para.style.name
            style_count[style_name] = style_count.get(style_name, 0) + 1
        
        return dict(sorted(style_count.items(), key=lambda x: x[1], reverse=True))
    
    def search_in_document(self, file_path, search_term):
        """åœ¨æ–‡æ¡£ä¸­æœç´¢ç‰¹å®šå†…å®¹"""
        try:
            doc = Document(file_path)
            results = []
            
            for i, para in enumerate(doc.paragraphs):
                if search_term.lower() in para.text.lower():
                    results.append({
                        "æ®µè½å·": i + 1,
                        "å†…å®¹": para.text.strip(),
                        "æ ·å¼": para.style.name
                    })
            
            return {
                "æœç´¢è¯": search_term,
                "æ‰¾åˆ°ç»“æœ": len(results),
                "åŒ¹é…å†…å®¹": results
            }
            
        except Exception as e:
            return {"é”™è¯¯": f"æœç´¢å¤±è´¥: {str(e)}"}
    
    def extract_text_only(self, file_path):
        """ä»…æå–æ–‡æ¡£çš„çº¯æ–‡æœ¬å†…å®¹"""
        try:
            doc = Document(file_path)
            text_content = []
            
            for para in doc.paragraphs:
                if para.text.strip():
                    text_content.append(para.text.strip())
            
            return {
                "çº¯æ–‡æœ¬å†…å®¹": text_content,
                "æ€»æ®µè½æ•°": len(text_content),
                "æ€»å­—ç¬¦æ•°": sum(len(text) for text in text_content)
            }
            
        except Exception as e:
            return {"é”™è¯¯": f"æå–æ–‡æœ¬å¤±è´¥: {str(e)}"}

def main():
    """æ¼”ç¤ºåŠŸèƒ½"""
    print("ğŸ” Cursor Wordæ–‡æ¡£åŠ©æ‰‹")
    print("=" * 50)
    
    helper = CursorWordHelper()
    
    # 1. æŸ¥æ‰¾Wordæ–‡æ¡£
    word_docs = helper.find_word_documents()
    
    if not word_docs:
        print("âŒ æœªæ‰¾åˆ°Wordæ–‡æ¡£")
        return
    
    print(f"ğŸ“„ æ‰¾åˆ° {len(word_docs)} ä¸ªWordæ–‡æ¡£:")
    for doc in word_docs:
        print(f"   ğŸ“ {doc['name']} ({doc['size_kb']} KB)")
        print(f"      è·¯å¾„: {doc['path']}")
    
    # 2. åˆ†æç¬¬ä¸€ä¸ªæ–‡æ¡£
    if word_docs:
        target_doc = word_docs[0]
        print(f"\nğŸ” æ­£åœ¨åˆ†æ: {target_doc['name']}")
        print("-" * 40)
        
        analysis = helper.analyze_document(target_doc['full_path'])
        print("âœ… åˆ†æå®Œæˆï¼")
        print(json.dumps(analysis, ensure_ascii=False, indent=2))

if __name__ == "__main__":
    main() 